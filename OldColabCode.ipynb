{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZB4171ImageClassificationProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjsJM8Vd-ygM"
      },
      "source": [
        "# **Import Libraries** #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwIXkRDa-yOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09b673e-dfd3-4fa9-b1e3-3b56bc9abbe3"
      },
      "source": [
        "#For Import Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "#For Data Augmentation\n",
        "import os\n",
        "import torch\n",
        "# import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# For Datasets and Dataloader\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.io import read_image\n",
        "\n",
        "#For EfficientNet Model Architecture\n",
        "! pip install efficientnet_pytorch\n",
        "import pandas as pd\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "#For ensemble model\n",
        "import collections\n",
        "\n",
        "#For Training & Validation\n",
        "import time\n",
        "import copy\n",
        "#import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
            "  from IPython.utils import traitlets as _traitlets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/bin/pip\", line 7, in <module>\r\n",
            "    from pip._internal.cli.main import main\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\r\n",
            "    from pip._internal.cli.autocompletion import autocomplete\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/cli/autocompletion.py\", line 9, in <module>\r\n",
            "    from pip._internal.cli.main_parser import create_main_parser\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\r\n",
            "    from pip._internal.cli import cmdoptions\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\r\n",
            "    from pip._internal.cli.progress_bars import BAR_TYPES\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/cli/progress_bars.py\", line 12, in <module>\r\n",
            "    from pip._internal.utils.logging import get_indentation\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 18, in <module>\r\n",
            "    from pip._internal.utils.misc import ensure_dir\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/utils/misc.py\", line 33, in <module>\r\n",
            "    from pip._internal.locations import (\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/locations/__init__.py\", line 9, in <module>\r\n",
            "    from . import _distutils, _sysconfig\r\n",
            "  File \"/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/locations/_sysconfig.py\", line 8, in <module>\r\n",
            "    from pip._internal.exceptions import InvalidSchemeCombination, UserInstallationInvalid\r\n",
            "ImportError: cannot import name 'InvalidSchemeCombination' from 'pip._internal.exceptions' (/home/tanwp/anaconda3/envs/ALLClassification/lib/python3.8/site-packages/pip/_internal/exceptions.py)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMAdhYhVmiqz"
      },
      "source": [
        "#**Directories & Folders**\n",
        "Directories created very slowly, may need to wait for folders to appear. Refresh files to allow folders to be updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avxKOeiRQqum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252c3918-c973-43c0-9bac-d109d56348fc"
      },
      "source": [
        "#Directory to store base EfficientNet model information\n",
        "!mkdir /content/baseEfficientNet\n",
        "!mkdir /content/baseEfficientNet/training_results\n",
        "!mkdir /content/baseEfficientNet/training_results/weights #Weights\n",
        "!mkdir /content/baseEfficientNet/training_results/stats #Training stats\n",
        "!mkdir /content/baseEfficientNet/testing_results/baseStats #Testing stats for base models\n",
        "!mkdir /content/baseEfficientNet/testing_results/ensembleStats #Testing stats for ensemble model\n",
        "\n",
        "#Directory to store Noisy Student model information\n",
        "!mkdir /content/noisyStudent\n",
        "!mkdir /content/noisyStudent/training_results\n",
        "!mkdir /content/noisyStudent/training_results/weights #Weights\n",
        "!mkdir /content/noisyStudent/training_results/stats #Training stats\n",
        "!mkdir /content/noisyStudent/testing_results/baseStats #Testing stats for base models\n",
        "!mkdir /content/noisyStudent/testing_results/ensembleStats #Testing stats for ensemble model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/baseEfficientNet/testing_results/baseStats’: No such file or directory\n",
            "mkdir: cannot create directory ‘/content/baseEfficientNet/testing_results/ensembleStats’: No such file or directory\n",
            "mkdir: cannot create directory ‘/content/noisyStudent/testing_results/baseStats’: No such file or directory\n",
            "mkdir: cannot create directory ‘/content/noisyStudent/testing_results/ensembleStats’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y7ttqOyAWz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8116f7-ba98-4b3f-e90f-c1c94b05329e"
      },
      "source": [
        "#Mount google drive to store datasets and results in.\n",
        "#Ensure data_main.zip and weights.zip files are uploaded into google driver folder first.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhXY_Kze25mX"
      },
      "source": [
        "# **Import Datasets** # \n",
        "Make sure to upload data_main.zip and weights.zip into google drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmjz4ua2ZXJ"
      },
      "source": [
        "# unzip data_main folder\n",
        "!unzip /content/drive/MyDrive/Data_main.zip -d ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGN1Xycd7lM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a02eb1-c938-4157-ad19-2cf8a76774e5"
      },
      "source": [
        "##Run this cell to debug and check datasets.\n",
        "\n",
        "#subset for debugging\n",
        "#!unzip /content/drive/MyDrive/Data_main_subset.zip -d .\n",
        "\n",
        "# check no. of images in train_images\n",
        "# !ls Data_main/images | wc -l\n",
        "\n",
        "# check no. of images in test_images \n",
        "#!ls Data_main/held_out_test/test_images | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5e8RejP9F8"
      },
      "source": [
        "# **Classes & Class Utils** #\n",
        "Defines custom class for image datasets and base EfficientNet architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv8x2xU3GiOS"
      },
      "source": [
        "##CellsDataset Class##\n",
        "Custom class for image dataset, required for usage of dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQcXP8RjP3vu"
      },
      "source": [
        "#Custom CellsDataset class for dataloader to process images.\n",
        "class CellsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, label_file, img_dir):\n",
        "        # initialise image directory\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "        # initialise labels\n",
        "        try:\n",
        "            self.img_labels = pd.read_csv(label_file) #Label csv files.\n",
        "        except:\n",
        "            self.img_labels = label_file #Label df files.\n",
        "        \n",
        "        # remove rows without an image path\n",
        "        files = os.listdir(self.img_dir)\n",
        "        self.img_labels = self.img_labels.loc[self.img_labels['Patient_ID'].isin(files)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
        "        \n",
        "        #Normalises images according to pretrained EfficientNet-B0 values\n",
        "        transform_data = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "        # image = read_image(img_path)\n",
        "        image = Image.open(img_path)\n",
        "        image = transform_data(image)\n",
        "        label = self.img_labels.iloc[idx, 2]\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFE00IKFIcfX"
      },
      "source": [
        "##EfficientNet Class##\n",
        "Custom class for base EfficientNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Zo3xXvcuUz"
      },
      "source": [
        "#Custom class for EfficientNet model.\n",
        "class BaseEfficientNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BaseEfficientNet,self).__init__()\n",
        "    #Import pre-trained b0 efficientnet - loads pre-trained weights for base model\n",
        "    self.model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.fc = nn.Linear(1280, 2) \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.float()\n",
        "    x = self.model.extract_features(x)\n",
        "    # print(f\"No of features: {x.shape}\")\n",
        "    x = self.avgpool(x)\n",
        "    x = x.flatten(start_dim=1)\n",
        "    x = self.dropout(x) #Randomly remove nodes to prevent overfitting\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1XIEhTcOBnN"
      },
      "source": [
        "## Ensemble Model Utils ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNqaKhHmHCjp"
      },
      "source": [
        "#Function to create an Ensemble model, returns a dictionary of (model name, model weights).\n",
        "def createEnsemble(model_class, weights_path):\n",
        "#Pull all files containing weights from each training fold.\n",
        "    weight_path = weights_path\n",
        "    file_paths = os.listdir(weight_path)\n",
        "\n",
        "    #Retrieve weights of each base model.\n",
        "    model_weights = list()\n",
        "    for pth in file_paths:\n",
        "        if 'pth' not in pth:\n",
        "          continue\n",
        "        path = f\"{weight_path}/{pth}\"\n",
        "        model = model_class\n",
        "        model.load_state_dict(torch.load(path, map_location = torch.device('cpu')), strict = False)\n",
        "        model_weights.append(model)\n",
        "\n",
        "    #Return a dictionary of the model weights\n",
        "    models = dict()\n",
        "    # for i in range(len(model_weights)):\n",
        "    #   models[f'model{i+1}'] = model_weights[i]\n",
        "\n",
        "    models['model1'] = model_weights[0]\n",
        "    models['model2'] = model_weights[1]\n",
        "    models['model3'] = model_weights[2]\n",
        "    models['model4'] = model_weights[3]\n",
        "    models['model5'] = model_weights[4]\n",
        "\n",
        "    #Delete intermediate variables to save memory\n",
        "    del model_weights\n",
        "    del model\n",
        "\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCHBUfE_q20S"
      },
      "source": [
        "#Function to predict using ensemble model via max Voting classification.\n",
        "def get_ensembleVoting(ensemble_model, test_input):\n",
        "  #Initialise base models of ensemble model.\n",
        "  model1 = ensemble_model['model1']\n",
        "  model2 = ensemble_model['model2']\n",
        "  model3 = ensemble_model['model3']\n",
        "  model4 = ensemble_model['model4']\n",
        "  model5 = ensemble_model['model5']\n",
        "  \n",
        "  #Send models to CPU/GPU \n",
        "  model1.to(device)\n",
        "  model2.to(device)\n",
        "  model3.to(device)\n",
        "  model4.to(device)\n",
        "  model5.to(device)\n",
        "\n",
        "  #Set model to evaluation mode\n",
        "  model1.eval()\n",
        "  model2.eval()\n",
        "  model3.eval()\n",
        "  model4.eval()\n",
        "  model5.eval()\n",
        "  \n",
        "  #Obtain base model predictions.\n",
        "  preds = list()\n",
        "  model1_output = model1(test_input)\n",
        "  _, model1_pred = torch.max(model1_output.data, 1)\n",
        "  preds.append(model1_pred)\n",
        "\n",
        "  model2_output = model2(test_input)\n",
        "  _, model2_pred = torch.max(model2_output.data, 1)\n",
        "  preds.append(model2_pred)\n",
        "\n",
        "  model3_output = model3(test_input)\n",
        "  _, model3_pred = torch.max(model3_output.data, 1)\n",
        "  preds.append(model3_pred)\n",
        "\n",
        "  model4_output = model4(test_input)\n",
        "  _, model4_pred = torch.max(model4_output.data, 1)\n",
        "  preds.append(model4_pred)\n",
        "\n",
        "  model5_output = model5(test_input)\n",
        "  _, model5_pred = torch.max(model5_output.data, 1)\n",
        "  preds.append(model5_pred)\n",
        "\n",
        "  #Find vote that has the maximum number and output max Voting prediction.\n",
        "  votes = collections.Counter(preds) #Counts frequency of each vote\n",
        "  common_vote = votes.most_common(1) #Retrive most common vote\n",
        "  final_pred = common_vote[0][0]\n",
        "\n",
        "  return final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJWxC2x4E92A"
      },
      "source": [
        "# **Training Base Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpvcMMioFnts"
      },
      "source": [
        "## **Initialise Training Parameters** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6wWPxXsFr4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1dcfd1-7f5a-43d7-d4b8-c6fbd129bde0"
      },
      "source": [
        "#Training parameters.\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9 # SGD momentum\n",
        "step_size = 7 # scheduler step size\n",
        "gamma = 0.1 # learning rate decay\n",
        "\n",
        "#EfficientNet and Noisy Student parameters.\n",
        "output_layer = 2\n",
        "criterion = nn.CrossEntropyLoss() #Loss function.\n",
        "seed = torch.manual_seed(42)\n",
        "num_fold = 5 # fix at 5\n",
        "batch_size = 5\n",
        "num_workers = 8 # to change based on machine used\n",
        "aug_rounds = 5\n",
        "CV_states = 42\n",
        "epochs = 100 # to change\n",
        "patience = 50 # add patience for early stopping\n",
        "\n",
        "#Use CUDA to run training, else use CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device in use: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device in use: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXCiIPC02NR"
      },
      "source": [
        "##**Load Pre-Trained EfficientNet Model**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLNmY6-B0170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cdb0a11-1b46-4ccb-94bb-83a6b168d51a"
      },
      "source": [
        "#Specify EfficientNet model as base model.\n",
        "base_model = BaseEfficientNet()\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "#Optimize with Stochastic Gradient descent\n",
        "optimizer = optim.SGD(base_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "#Decay the learning rate.\n",
        "base_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9vbQZ0rFc9y"
      },
      "source": [
        "## **Generate Folds for K-Fold CV** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnPgVJOi0hGJ"
      },
      "source": [
        "#Initialise labels file.\n",
        "label_csv = \"Data_main/train_labels.csv\"\n",
        "\n",
        "# Define stratified K-fold cross-validation function.\n",
        "strat_kfold = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=CV_states)\n",
        "\n",
        "#Obtain indexes for stratified k fold.\n",
        "label_df = pd.read_csv(label_csv)\n",
        "X = np.zeros(len(label_df))\n",
        "y = label_df['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzdy4swVP10_"
      },
      "source": [
        "## **Training base EfficientNet Model** ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1rRU1uUTKjd"
      },
      "source": [
        "# Base model output directories\n",
        "weight_path = \"baseEfficientNet/training_results/weights\"\n",
        "stats_path = \"baseEfficientNet/training_results/stats\"\n",
        "version = 'noisy_debug' # change every round to prevent overriding of weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOiyt7DRA8C2"
      },
      "source": [
        "#Training Function.\n",
        "def train_baseModel(model, criterion, optimizer, scheduler, num_epochs=epochs, fold=0):\n",
        "    \n",
        "    # specify output files\n",
        "    log_file = f\"{stats_path}/fold{fold+1}_{version}_log.log\"\n",
        "    weight_file = f\"{weight_path}/fold{fold+1}_{version}_weights.pth\"\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    #Initialise model.\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_no = epoch+1\n",
        "        print('Epoch {}/{}'.format(epoch_no, num_epochs))\n",
        "        print('-' * 10)\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Epoch {}/{}, '.format(epoch_no, num_epochs))\n",
        "        \n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['training', 'validation']:\n",
        "            if phase == 'training':\n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                model.eval()   # Set model to validation mode\n",
        "\n",
        "            p_count = 0\n",
        "            lowest_loss = 0.0\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                #Set parameter gradients to 0.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #Iterate forward.\n",
        "                #Track history only if in training phase.\n",
        "                with torch.set_grad_enabled(phase == 'training'):\n",
        "                    outputs = model(inputs)\n",
        "                    soft_preds = torch.softmax(outputs, dim = -1) #Generate soft labels\n",
        "                    max_prob, hard_preds = torch.max(soft_preds, dim = -1)\n",
        "                    loss = criterion(soft_preds, labels)\n",
        "\n",
        "                    #Backward propagate. \n",
        "                    #Update parameters only if in training phase.\n",
        "                    if phase == 'training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #Output training statistics.\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(hard_preds == labels.data)\n",
        "            \n",
        "            if phase == 'training':\n",
        "                scheduler.step()\n",
        "\n",
        "            #Calculates epoch statistics.\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            with open(log_file, 'a') as log:\n",
        "              log.write('{} Loss: {:.4f} Acc: {:.4f}, '.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "              \n",
        "            #Validation phase\n",
        "            if phase == 'validation':\n",
        "              with open(log_file, 'a') as log:\n",
        "                  log.write('\\n')\n",
        "\n",
        "              #Add patience for early stopping\n",
        "              if (lowest_loss <= epoch_loss):\n",
        "                p_count += 1\n",
        "                if p_count > patience:\n",
        "                  print(f'val loss did not decrease after {patience} epochs')\n",
        "                  break\n",
        "\n",
        "              if lowest_loss > epoch_loss:\n",
        "                p_count = 0\n",
        "                lowest_loss = epoch_loss\n",
        "\n",
        "              #Save best model weights if epoch gives best accuracy\n",
        "              if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_epoch = epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        epoch_end = time.time()\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        print('Epoch Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "\n",
        "        print()\n",
        "    \n",
        "    #Output runtime of model.\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Time {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    with open(log_file, 'a') as log:\n",
        "      log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          time_elapsed // 60, time_elapsed % 60))\n",
        "      log.write('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    #Save weights of the best epoch model into training directory.\n",
        "    torch.save(best_model_wts, weight_file)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cxR7lc-P0ZV"
      },
      "source": [
        "#Fold code.\n",
        "for fold, (train_idx, valid_idx) in enumerate(strat_kfold.split(X, y)):\n",
        "    fold_no = fold+1\n",
        "    print(f\"FOLD {fold_no}\")\n",
        "\n",
        "    base_model = BaseEfficientNet()\n",
        "    base_model = base_model.to(device)\n",
        "    optimizer = optim.SGD(base_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    base_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma) \n",
        "\n",
        "    train_df = label_df.iloc[train_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    valid_df = label_df.iloc[valid_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    train_set = CellsDataset(train_df, \"Data_main/images\")\n",
        "    valid_set = CellsDataset(valid_df, \"Data_main/images\")\n",
        "\n",
        "    #Add augmentation data to original training set.\n",
        "    # aug_dir = \"../test_augment/\"\n",
        "    # aug_df = augment(rounds=aug_rounds, op_dir=aug_dir, labels=train_df)\n",
        "    # aug_set = CellsDataset(aug_df, aug_dir)\n",
        "    # train_aug_set = torch.utils.data.ConcatDataset([train_set, aug_set])\n",
        "\n",
        "    #Load dataset onto dataloader\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "    dataloaders = {'training': train_loader, 'validation': valid_loader}\n",
        "    dataset_sizes = {'training': len(train_set), 'validation': len(valid_set)}\n",
        "    class_names = [0, 1] # hard code labels based on labels in csv\n",
        "    \n",
        "    #train data on n epochs\n",
        "    #trained_baseModel = train_baseModel(base_model, criterion, optimizer, base_scheduler, num_epochs=epochs, fold=fold)\n",
        "\n",
        "    #Delete augmented training directory before next training fold to save disk space.\n",
        "    #shutil.rmtree(aug_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7II0EPGj04BY"
      },
      "source": [
        "## **Training Noisy Student** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNYzYjuoxON"
      },
      "source": [
        "# Noisy student output directories\n",
        "weight_path = \"/content/noisyStudent/training_results/weights\"\n",
        "stats_path = \"/content/noisyStudent/training_results/stats\"\n",
        "#version = 'v2' # change every round to prevent overriding of weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xqxCKPd6SJ8"
      },
      "source": [
        "#Training Function for noisy student.\n",
        "def noisyStudentTraining(model, dataloader, dataset_sizes, criterion, optimizer, scheduler, stats_path, weight_path, num_epochs = 50, fold = 0):\n",
        "    \n",
        "    # specify output files\n",
        "    log_file = f\"{stats_path}/fold{fold+1}_{version}_log.log\"\n",
        "    weight_file = f\"{weight_path}/fold{fold+1}_{version}_weights.pth\"\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    #Initialise model.\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_no = epoch+1\n",
        "        print('Epoch {}/{}'.format(epoch_no, num_epochs))\n",
        "        print('-' * 10)\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Epoch {}/{}, '.format(epoch_no, num_epochs))\n",
        "        \n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['training', 'validation']:\n",
        "            if phase == 'training':\n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                model.eval()   # Set model to validation mode\n",
        "\n",
        "            p_count = 0\n",
        "            lowest_loss = 0.0\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                #Set parameter gradients to 0.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #Iterate forward.\n",
        "                #Track history only if in training phase.\n",
        "                with torch.set_grad_enabled(phase == 'training'):\n",
        "                    outputs = model(inputs)\n",
        "                    soft_preds = torch.softmax(outputs, dim = -1) #Generate soft labels\n",
        "                    max_prob, hard_preds = torch.max(soft_preds, dim = -1)\n",
        "                    loss = criterion(soft_preds, labels)\n",
        "\n",
        "                    #Backward propagate. \n",
        "                    #Update parameters only if in training phase.\n",
        "                    if phase == 'training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #Output training statistics.\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(hard_preds == labels.data)\n",
        "            \n",
        "            if phase == 'training':\n",
        "                scheduler.step()\n",
        "\n",
        "            #Calculates epoch statistics.\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            with open(log_file, 'a') as log:\n",
        "              log.write('{} Loss: {:.4f} Acc: {:.4f}, '.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "              \n",
        "            #Validation phase\n",
        "            if phase == 'validation':\n",
        "              with open(log_file, 'a') as log:\n",
        "                  log.write('\\n')\n",
        "\n",
        "              #Add patience for early stopping\n",
        "              if (lowest_loss <= epoch_loss):\n",
        "                p_count += 1\n",
        "                if p_count > patience:\n",
        "                  print(f'val loss did not decrease after {patience} epochs')\n",
        "                  break\n",
        "\n",
        "              if lowest_loss > epoch_loss:\n",
        "                p_count = 0\n",
        "                lowest_loss = epoch_loss\n",
        "\n",
        "              #Save best model weights if epoch gives best accuracy\n",
        "              if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_epoch = epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        epoch_end = time.time()\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        print('Epoch Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "\n",
        "        print()\n",
        "    \n",
        "    #Output runtime of model.\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Time {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    with open(log_file, 'a') as log:\n",
        "      log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          time_elapsed // 60, time_elapsed % 60))\n",
        "      log.write('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    #Save weights of the best epoch model into training directory.\n",
        "    torch.save(best_model_wts, weight_file)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukY3zI11DJC"
      },
      "source": [
        "#Prepare data for training on each fold for noisy student model.\n",
        "for fold, (train_idx, valid_idx) in enumerate(strat_kfold.split(X, y)):\n",
        "\n",
        "    print(\"FOLD {}\".format(fold+1))\n",
        "    \n",
        "    base_model = BaseEfficientNet()\n",
        "    base_model = base_model.to(device)\n",
        "    optimizer = optim.SGD(base_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    base_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma) \n",
        "    \n",
        "    #Split training into labelled(50%)/unlabelled(50%)\n",
        "    labelled_idx = []\n",
        "    unlabelled_idx = []\n",
        "    a = train_idx\n",
        "    for idx in range(len(train_idx)):\n",
        "      index = train_idx[idx]\n",
        "      \n",
        "      if (index % 2 == 0):\n",
        "        labelled_idx.append(index)\n",
        "      else:\n",
        "        unlabelled_idx.append(index)\n",
        "    \n",
        "    labelled_df = label_df.iloc[labelled_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    unlabelled_df = label_df.iloc[unlabelled_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    valid_df = label_df.iloc[valid_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "\n",
        "    # to change directory\n",
        "    valid_set = CellsDataset(valid_df, \"Data_main/images\")\n",
        "    labelled_set = CellsDataset(labelled_df, \"Data_main/images\")\n",
        "    unlabelled_set = CellsDataset(unlabelled_df, \"Data_main/images\")\n",
        "\n",
        "    #Load dataset onto dataloader\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\n",
        "    label_loader = torch.utils.data.DataLoader(labelled_set, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\n",
        "    unlabel_loader = torch.utils.data.DataLoader(unlabelled_set, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\n",
        "    \n",
        "    \n",
        "    label_dataloader = {'training': label_loader, 'validation': valid_loader}\n",
        "    dataset_sizes_l = {'training': len(labelled_set), 'validation': len(valid_set)}\n",
        "    class_names = [0, 1] # hard code labels based on labels in csv\n",
        "    \n",
        "    #Training noisy student model.\n",
        "    models = 3\n",
        "    #Update variable with unlabelled images and inferred pseudolabels\n",
        "    pseudolabel_df = unlabelled_df[[\"Patient_no\", \"Patient_ID\"]]\n",
        "\n",
        "    for loop in range(models):\n",
        "      if loop == 0:\n",
        "        #Training first teacher model on original labelled data.\n",
        "        print(\"TRAINING TEACHER:\")\n",
        "        noisyStudentTraining(base_model, label_dataloader, dataset_sizes_l, criterion, optimizer, base_scheduler, stats_path, weight_path, num_epochs = 50, fold=fold)\n",
        "\n",
        "      else:\n",
        "        #Obtain predictions on unlabelled data to obtain pseudolabels.\n",
        "        base_model.eval()\n",
        "\n",
        "        infer_pseudolabels = []\n",
        "        #Generate pseudolabels from unlabelled data\n",
        "        for inputs, _ in unlabel_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            with torch.no_grad():\n",
        "              output = base_model(inputs)\n",
        "              soft_preds = torch.softmax(output, dim = -1)\n",
        "              max_prob, hard_preds = torch.max(soft_preds, dim = -1)\n",
        "              hard_preds = hard_preds.cpu().detach().numpy()\n",
        "              infer_pseudolabels.extend(hard_preds)\n",
        "\n",
        "        #Add pseudolabel column from inferred pseudolabels\n",
        "        pseudolabel_df['labels'] = infer_pseudolabels\n",
        "        # pseudolabel_df = unlabelled_df[['Patient_no', 'Patient_ID']].concat(infer_pseudolabels)\n",
        "        pseudolabel_set = CellsDataset(pseudolabel_df, \"Data_main_subset/images\")\n",
        "        pseudolabel_loader = torch.utils.data.DataLoader(pseudolabel_set, batch_size=batch_size, shuffle=True)\n",
        "        pseudo_dataloader = {'training': pseudolabel_loader, 'validation': valid_loader}\n",
        "        dataset_sizes_u = {'training': len(pseudolabel_set), 'validation': len(valid_set)}\n",
        "\n",
        "        #Train student models on pseudolabelled data.\n",
        "        print(f\"TRAINING STUDENT LOOP: {loop}\")\n",
        "        noisyStudentTraining(base_model, pseudo_dataloader, dataset_sizes_u, criterion, optimizer, base_scheduler, stats_path, weight_path, num_epochs = 50, fold=fold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FwLc0PAKtsn"
      },
      "source": [
        "## **Create Ensemble Models** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oxc4-cMIlwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd14b127-b26c-48ec-e849-f2799b74ae72"
      },
      "source": [
        "#Load weights of base models for ensemble models.\n",
        "# !unzip /content/drive/MyDrive/8Nov_weight.zip -d /content/baseEfficientNet/training_results/weights #base EfficientNet Model\n",
        "# baseEfficientNet_weights = \"/content/baseEfficientNet/training_results/weights/8Nov_weight\"\n",
        "baseEfficientNet_weights = \"NoisyStudent/weights/10Nov_Noisy_weight\"\n",
        "#Create ensemble models.\n",
        "base_ensembleModel = createEnsemble(BaseEfficientNet(), baseEfficientNet_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiAT98O8qwjj"
      },
      "source": [
        "# **Testing and Evaluation** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCYXoY-VOH2l"
      },
      "source": [
        "## **Testing Function** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hroUC0mrOBXl"
      },
      "source": [
        "#Testing function.\n",
        "def test_model(model, stats_path, ensemble=False, fold = ''):\n",
        "  #Message to indicate if evaluating base or ensemble model.\n",
        "  if ensemble == False:\n",
        "    print(\"Evaluating base models...\")\n",
        "  else:\n",
        "    print(\"Evaluating ensemble model...\")\n",
        "  \n",
        "  #Create empty confusion matrix.\n",
        "  confusion_matrix = np.zeros((2,2), dtype=int)\n",
        "  \n",
        "  with torch.no_grad(): #Disable backpropagation\n",
        "    for inputs, labels in test_dataloader:  #Iterate over testing data.\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "    \n",
        "      if ensemble == False: #Testing base model\n",
        "          output = model(inputs)\n",
        "          _, prediction = torch.max(output.data, 1)\n",
        "\n",
        "      if ensemble == True: #Testing ensemble model\n",
        "          prediction = get_ensembleVoting(model, inputs)\n",
        "\n",
        "      #Evaluate predictions and summarise in confusion matrix.\n",
        "      for j in range(inputs.size()[0]): \n",
        "          if prediction[j]==1 and labels[j]==1:\n",
        "              term='TP'\n",
        "              confusion_matrix[0][0]+=1\n",
        "\n",
        "          elif prediction[j]==1 and labels[j]==0:\n",
        "              term='FP'\n",
        "              confusion_matrix[1][0]+=1\n",
        "\n",
        "          elif prediction[j]==0 and labels[j]==1:\n",
        "              term='FN'\n",
        "              confusion_matrix[0][1]+=1\n",
        "        \n",
        "          elif prediction[j]==0 and labels[j]==0:\n",
        "              term='TN'\n",
        "              confusion_matrix[1][1]+=1\n",
        "\n",
        "      #Obtain results from confusion matrix.\n",
        "      TP = confusion_matrix[0][0]\n",
        "      FP = confusion_matrix[0][1]\n",
        "      FN = confusion_matrix[1][0]\n",
        "      TN = confusion_matrix[1][1]\n",
        "\n",
        "      #Calculate test statistics.\n",
        "      accuracy = 100*(TP+TN)/ (TP+FP+TN+FN)\n",
        "      sensitivity = (100*TP)/(TP+FP)\n",
        "      specificity = (100*TN)/(TN+FN)\n",
        "      PPV = (100*TP)/(TP+FN)\n",
        "      NPV = (100*TN)/(TN+FP)\n",
        "      F1 = 2*(PPV*sensitivity)/(PPV+sensitivity) \n",
        "\n",
        "    #Print test statistics.\n",
        "    print('-----------------------')\n",
        "    print('PREDICTION STATISTICS')\n",
        "    print('-----------------------')\n",
        "    print('True Positives: ' + str(TP))\n",
        "    print('False Positives: ' + str(FP))\n",
        "    print('False Negatives: ' + str(FN))\n",
        "    print('True Negatives: ' + str(TN))\n",
        "\n",
        "    print('-----------------------')\n",
        "    print('EVALUATION STATISTICS')\n",
        "    print('-----------------------')\n",
        "    print('Accuracy: %f %%' % (accuracy))\n",
        "    print('Sensitivity: %f %%' % (sensitivity))\n",
        "    print('Specificity: %f %%' % (specificity))\n",
        "    print('PPV: %f %%' % (PPV))\n",
        "    print('NPV: %f %%' % (NPV))\n",
        "    print('F1 Score: %f %%' % (F1))\n",
        "\n",
        "    #Save statistics as log files.\n",
        "    stat_file = f\"{stats_path}_stats.log\"\n",
        "\n",
        "    if ensemble:\n",
        "      with open(stat_file, 'a') as log:\n",
        "        log.write('ENSEMBLE MODEL:' + '\\n')\n",
        "\n",
        "    else:\n",
        "      with open(stat_file, 'a') as log:\n",
        "        log.write(fold + '\\n')\n",
        "\n",
        "    #Save test statistics\n",
        "    with open(stat_file, 'a') as log:\n",
        "      log.write(\n",
        "      'True Positives: ' + str(TP) + '\\n'\n",
        "      'False Positives: ' + str(FP) + '\\n'\n",
        "      'False Negatives: ' + str(FN) + '\\n'\n",
        "      'True Negatives: ' + str(TN) + '\\n' + '\\n'\n",
        "      'Accuracy: %f %%' % (accuracy) +'\\n'\n",
        "      'Sensitivity: %f %%' % (sensitivity) + '\\n'\n",
        "      'Specificity: %f %%' % (specificity) + '\\n'\n",
        "      'PPV: %f %%' % (PPV) + '\\n'\n",
        "      'NPV: %f %%' % (NPV) + '\\n'\n",
        "      'F1 Score: %f %%' % (F1) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoiT57bT1TqL"
      },
      "source": [
        "## Evaluating ensembl models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwuleombgYWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcebbf4-2d9f-40ea-abef-8919b1ea5515"
      },
      "source": [
        "#Initialise testing set files\n",
        "test_path = \"Data_main/held_out_test/test_labels.csv\"\n",
        "test_df = pd.read_csv(test_path)[[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "test_df.index = test_df.index + 1\n",
        "test_set = CellsDataset(test_df, \"Data_main/held_out_test/test_images\")\n",
        "\n",
        "#Initialise paths for testing_stats\n",
        "baseStats_path = \"/content/baseEfficientNet/testing_results/baseStats\"\n",
        "# ensembleStats_path = \"/content/baseEfficientNet/testing_results/ensembleStats\"\n",
        "baseStats_path = \"NoisyStudent/stats\"\n",
        "ensembleStats_path = \"NoisyStudent/stats\"\n",
        "\n",
        "#Dataloader for testing input.\n",
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#Evaluate base models.\n",
        "# model1 = base_ensembleModel['model1']\n",
        "# model2 = base_ensembleModel['model2']\n",
        "# model3 = base_ensembleModel['model3']\n",
        "# model4 = base_ensembleModel['model4']\n",
        "# model5 = base_ensembleModel['model5']\n",
        "\n",
        "# test_basemodel1 = test_model(model1, stats_path=baseStats_path, ensemble=False)\n",
        "# test_basemodel2 = test_model(model2, stats_path=baseStats_path, ensemble=False)\n",
        "# test_basemodel3 = test_model(model3, stats_path=baseStats_path, ensemble=False)\n",
        "# test_basemodel4 = test_model(model4, stats_path=baseStats_path, ensemble=False)\n",
        "# test_basemodel5 = test_model(model5, stats_path=baseStats_path, ensemble=False)\n",
        "\n",
        "#Evaluate ensemble models.\n",
        "test_baseEnsembleModel = test_model(base_ensembleModel, stats_path=ensembleStats_path, ensemble=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ensemble model...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1760\n",
            "False Positives: 76\n",
            "False Negatives: 143\n",
            "True Negatives: 435\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 90.927920 %\n",
            "Sensitivity: 95.860566 %\n",
            "Specificity: 75.259516 %\n",
            "PPV: 92.485549 %\n",
            "NPV: 85.127202 %\n",
            "F1 Score: 94.142819 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EP7hmSyrJdp"
      },
      "source": [
        "## Evaluating Individual Fold's models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFLIauIEcn3f",
        "outputId": "1b70a0d3-1d7a-44b6-da87-a7b8b3dac269"
      },
      "source": [
        "# Noisy student outputs\n",
        "weight_dir = \"NoisyStudent/weights/10Nov_Noisy_weight\"\n",
        "\n",
        "\n",
        "weights = os.listdir(weight_dir)\n",
        "for weight in weights:\n",
        "  pth = weight_dir + '/'+ weight\n",
        "  model = BaseEfficientNet()\n",
        "  model.load_state_dict(torch.load(pth))\n",
        "  # load to gpu\n",
        "  model.to(device)\n",
        "\n",
        "  path = baseStats_path + '/' + weight.split('.')[0]\n",
        "  test_model(model, stats_path=path, ensemble=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1636\n",
            "False Positives: 200\n",
            "False Negatives: 138\n",
            "True Negatives: 440\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 85.998343 %\n",
            "Sensitivity: 89.106754 %\n",
            "Specificity: 76.124567 %\n",
            "PPV: 92.220970 %\n",
            "NPV: 68.750000 %\n",
            "F1 Score: 90.637119 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1642\n",
            "False Positives: 194\n",
            "False Negatives: 123\n",
            "True Negatives: 455\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.868268 %\n",
            "Sensitivity: 89.433551 %\n",
            "Specificity: 78.719723 %\n",
            "PPV: 93.031161 %\n",
            "NPV: 70.107858 %\n",
            "F1 Score: 91.196890 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-62fe5d850eb6>:51: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  specificity = (100*TN)/(TN+FN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1649\n",
            "False Positives: 187\n",
            "False Negatives: 127\n",
            "True Negatives: 451\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.992543 %\n",
            "Sensitivity: 89.814815 %\n",
            "Specificity: 78.027682 %\n",
            "PPV: 92.849099 %\n",
            "NPV: 70.689655 %\n",
            "F1 Score: 91.306755 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1612\n",
            "False Positives: 224\n",
            "False Negatives: 136\n",
            "True Negatives: 442\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 85.086993 %\n",
            "Sensitivity: 87.799564 %\n",
            "Specificity: 76.470588 %\n",
            "PPV: 92.219680 %\n",
            "NPV: 66.366366 %\n",
            "F1 Score: 89.955357 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1643\n",
            "False Positives: 193\n",
            "False Negatives: 135\n",
            "True Negatives: 443\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.412593 %\n",
            "Sensitivity: 89.488017 %\n",
            "Specificity: 76.643599 %\n",
            "PPV: 92.407199 %\n",
            "NPV: 69.654088 %\n",
            "F1 Score: 90.924184 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGGM8CxOHKpR",
        "outputId": "414adecf-2538-47b1-f2b1-9c2dcb7ecc13"
      },
      "source": [
        "# Base Model outputs\n",
        "weight_dir = \"/content/baseEfficientNet/training_results/weights/8Nov_weight\"\n",
        "baseStats_path = \"/content/baseEfficientNet/testing_results/baseStats\"\n",
        "\n",
        "weights = os.listdir(weight_dir)\n",
        "for weight in weights:\n",
        "  pth = weight_dir + '/'+ weight\n",
        "  model = BaseEfficientNet()\n",
        "  model.load_state_dict(torch.load(pth, map_location = torch.device('cpu')))\n",
        "\n",
        "  path = baseStats_path + '/' + weight.split('.')[0]\n",
        "  test_model(model, stats_path=path, ensemble=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1637\n",
            "False Positives: 199\n",
            "False Negatives: 127\n",
            "True Negatives: 451\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.495443 %\n",
            "Sensitivity: 89.161220 %\n",
            "Specificity: 78.027682 %\n",
            "PPV: 92.800454 %\n",
            "NPV: 69.384615 %\n",
            "F1 Score: 90.944444 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1643\n",
            "False Positives: 193\n",
            "False Negatives: 130\n",
            "True Negatives: 448\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.619718 %\n",
            "Sensitivity: 89.488017 %\n",
            "Specificity: 77.508651 %\n",
            "PPV: 92.667795 %\n",
            "NPV: 69.890796 %\n",
            "F1 Score: 91.050152 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1646\n",
            "False Positives: 190\n",
            "False Negatives: 135\n",
            "True Negatives: 443\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.536868 %\n",
            "Sensitivity: 89.651416 %\n",
            "Specificity: 76.643599 %\n",
            "PPV: 92.419989 %\n",
            "NPV: 69.984202 %\n",
            "F1 Score: 91.014653 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n",
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1633\n",
            "False Positives: 203\n",
            "False Negatives: 138\n",
            "True Negatives: 440\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 85.874068 %\n",
            "Sensitivity: 88.943355 %\n",
            "Specificity: 76.124567 %\n",
            "PPV: 92.207792 %\n",
            "NPV: 68.429238 %\n",
            "F1 Score: 90.546160 %\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Evaluating base models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-32049a5d0d44>:51: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  specificity = (100*TN)/(TN+FN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "PREDICTION STATISTICS\n",
            "-----------------------\n",
            "True Positives: 1637\n",
            "False Positives: 199\n",
            "False Negatives: 134\n",
            "True Negatives: 444\n",
            "-----------------------\n",
            "EVALUATION STATISTICS\n",
            "-----------------------\n",
            "Accuracy: 86.205468 %\n",
            "Sensitivity: 89.161220 %\n",
            "Specificity: 76.816609 %\n",
            "PPV: 92.433653 %\n",
            "NPV: 69.051322 %\n",
            "F1 Score: 90.767951 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWNxDk8krPEL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mfZxAmPonUX"
      },
      "source": [
        "# **Miscellaneous** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC5xLYA1isLw"
      },
      "source": [
        "##**Initialise Pre-Trained MPL Model**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTwfezooeWl"
      },
      "source": [
        "#MPL model\n",
        "# s_model, t_model = BaseEfficientNet(), BaseEfficientNet()\n",
        "# s_model, t_model = s_model.to(device), t_model.to(device)\n",
        "# #Optimize with Stochastic Gradient descent\n",
        "# s_optimizer = optim.SGD(s_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "# t_optimizer = optim.SGD(t_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "# #Decay the learning rate by a factor of 0.1 for every 7 epochs.\n",
        "# s_scheduler = StepLR(s_optimizer, step_size=step_size, gamma=gamma)\n",
        "# t_scheduler = StepLR(t_optimizer, step_size=step_size, gamma=gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdIHkgaWlyo8"
      },
      "source": [
        "##Ensembl EfficientNet Class##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnz9gB4WAnxA"
      },
      "source": [
        "#Custom class for Ensembl model using models generated from all training folds.\n",
        "class EnsblEfficientNet(nn.Module):\n",
        "  def __init__(self, models, nb_classes=2):\n",
        "    super(EnsblEfficientNet, self).__init__()\n",
        "    self.modelA = models[0]\n",
        "    self.modelB = models[1]\n",
        "    self.modelC = models[2]\n",
        "    self.modelD = models[3]\n",
        "    self.modelE = models[4]\n",
        "\n",
        "    # Remove last linear layer\n",
        "    # self.modelA.fc = nn.Identity()\n",
        "    # self.modelB.fc = nn.Identity()\n",
        "    # self.modelC.fc = nn.Identity()\n",
        "    # self.modelD.fc = nn.Identity()\n",
        "    # self.modelE.fc = nn.Identity()\n",
        "      \n",
        "    # Create new classifier\n",
        "    self.sub_classifier = nn.Linear(2, nb_classes) # classify hard labels\n",
        "    self.classifier = nn.Linear(2*5, nb_classes)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    #clone to make sure x is not changed by inplace methods\n",
        "    x1 = self.modelA(x.clone()) \n",
        "    x1 = x1.view(x1.size(0), -1)\n",
        "    x2 = self.modelB(x.clone())\n",
        "    x2 = x2.view(x2.size(0), -1)\n",
        "    x3 = self.modelC(x.clone())\n",
        "    x3 = x3.view(x3.size(0), -1)\n",
        "    x4 = self.modelD(x.clone())\n",
        "    x4 = x4.view(x4.size(0), -1)\n",
        "    x5 = self.modelE(x.clone())\n",
        "    x5 = x5.view(x5.size(0), -1)\n",
        "\n",
        "    x1 = self.sub_classifier(F.relu(x1))\n",
        "    x2 = self.sub_classifier(F.relu(x2))\n",
        "    x3 = self.sub_classifier(F.relu(x3))\n",
        "    x4 = self.sub_classifier(F.relu(x4))\n",
        "    x5 = self.sub_classifier(F.relu(x5))\n",
        "\n",
        "    #Concatenate models to form ensembl model.\n",
        "    x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
        "    #Obtain prediction.\n",
        "    x = self.classifier(F.relu(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKRy0Feessi7"
      },
      "source": [
        "## **Training Meta-Pseudo Label Model** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSWGB63En7ec"
      },
      "source": [
        "def train_metaModel(s_model, t_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler, num_epochs=epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    # Initialise model.\n",
        "    best_model_wts = copy.deepcopy(s_model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_no = epoch + 1\n",
        "        print('Epoch {}/{}'.format(epoch_no, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['training', 'validation']:\n",
        "            #Training phase\n",
        "            if phase == 'training':\n",
        "                s_model.train()  # Set model to training mode\n",
        "                t_model.train()\n",
        "\n",
        "                # iterate through size of labelled and unlabelled set\n",
        "                for i in range(len(labelled_set)):\n",
        "                    # load labelled and unlabelled dataloaders\n",
        "                    labelled_loader = dataloaders['training'][0]  # to put training dataloaders as a list in dataloaders dict\n",
        "                    unlabelled_loader = dataloaders['training'][1]\n",
        "\n",
        "                    # Labelled dataloader\n",
        "                    labelled_iter = iter(labelled_loader)\n",
        "                    images_l, targets = labelled_iter.next()\n",
        "                    images_l, targets = images_l.to(device), targets.to(device)\n",
        "                    batch_size = images_l.shape[0]\n",
        "\n",
        "                    # Unlabelled dataloader\n",
        "                    unlabelled_iter = iter(unlabelled_loader)\n",
        "                    images_u, _ = unlabelled_iter.next()\n",
        "                    images_u = images_u.to(device)\n",
        "\n",
        "                    # Initialise images for teacher and student\n",
        "                    all_images = torch.cat((images_l, images_u))\n",
        "\n",
        "                    # Set parameter gradients to 0.\n",
        "                    s_optimizer.zero_grad()\n",
        "                    t_optimizer.zero_grad()\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'training'):\n",
        "                        # Run initial training for teacher model\n",
        "                        t_outputs = t_model(all_images)\n",
        "                        # Split t_outputs based on labelled and unlabelled\n",
        "                        t_outputs_l = t_outputs[:batch_size]\n",
        "                        t_outputs_u = t_outputs[batch_size:]\n",
        "\n",
        "                        # Obtain training predictions on unlabelled\n",
        "                        soft_preds = torch.softmax(t_outputs_u, dim=-1)\n",
        "                        max_prob, pseudo_preds = torch.max(soft_preds, dim=-1)\n",
        "\n",
        "                        # Obtain teacher's loss on labelled\n",
        "                        t_loss_l = criterion(t_outputs_l, targets)\n",
        "\n",
        "                        # Run training for student model\n",
        "                        s_outputs = s_model(all_images)\n",
        "                        # Split s_outputs based on labelled and unlabelled\n",
        "                        s_outputs_l = s_outputs[:batch_size]\n",
        "                        s_outputs_u = s_outputs[batch_size:]\n",
        "\n",
        "\n",
        "                        # Obtain loss on labelled\n",
        "                        s_loss_l_old = criterion(s_outputs_l, targets)\n",
        "\n",
        "                        # Obtain loss on unlabelled + backward propagate to update parameters\n",
        "                        s_loss = criterion(s_outputs_u, pseudo_preds)\n",
        "                        s_loss.backward()\n",
        "                        s_optimizer.step()\n",
        "                        # s_scheduler.scale(s_loss).backward()\n",
        "                        # clip_gradient(s_optimizer, grad_clip) #Clip gradient to prevent exploding gradients\n",
        "                        # s_scaler.step(s_optimizer)\n",
        "                        # s_scaler.update()\n",
        "                        # s_scheduler.step()\n",
        "\n",
        "                        # Get dot product to feedback with teacher's unlabelled prediction\n",
        "                        s_outputs_l_new = s_model(images_l)\n",
        "                        s_loss_l_new = criterion(s_outputs_l_new, targets)\n",
        "                        dot_product = s_loss_l_old.detach() - s_loss_l_new.detach()\n",
        "\n",
        "                        # Calculate total teacher's loss\n",
        "                        t_loss_mpl = criterion(t_outputs_u.detach(), pseudo_preds.detach()) * dot_product\n",
        "                        t_loss = t_loss_l + t_loss_mpl\n",
        "\n",
        "                        # Update teacher parameters based on loss (feedback loop)\n",
        "                        t_loss.backward()\n",
        "                        t_optimizer.step()\n",
        "                        # t_scaler.scale(t_loss).backward()\n",
        "                        # clip_gradient(t_optimizer, grad_clip) #Clip gradient to prevent exploding gradients\n",
        "                        # t_scaler.step(t_optimizer)\n",
        "                        # t_scaler.update()\n",
        "                        # t_scheduler.step()\n",
        "\n",
        "                s_scheduler.step()\n",
        "                t_scheduler.step()\n",
        "\n",
        "            # Validation phase.\n",
        "            if phase == 'validation':\n",
        "                s_model.eval()  # Set model to validation mode\n",
        "                # t_model.eval()\n",
        "\n",
        "                p_count = 0\n",
        "                lowest_loss = 0.0\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Run prediction for student model using validation set.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    outputs = s_model(inputs)\n",
        "                    \n",
        "                    soft_preds = torch.softmax(outputs, dim=-1)  # Generate soft labels\n",
        "                    max_prob, hard_preds = torch.max(soft_preds, dim=-1)\n",
        "                    loss = criterion(soft_preds, labels)\n",
        "\n",
        "                    # Output training statistics.\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(hard_preds == labels.data)\n",
        "\n",
        "                # Calculates epoch statistics.\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                    phase, epoch_loss, epoch_acc))\n",
        "\n",
        "                # Add patience for early stopping\n",
        "                if (lowest_loss <= epoch_loss):\n",
        "                    p_count += 1\n",
        "                    if p_count > patience:\n",
        "                        print(f'val loss did not decrease after {patience} epochs')\n",
        "                        break\n",
        "\n",
        "                if lowest_loss > epoch_loss:\n",
        "                    p_count = 0\n",
        "                    lowest_loss = epoch_loss\n",
        "                    \n",
        "                # Save best model weights if epoch gives best accuracy\n",
        "                if epoch_acc >= best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_epoch = epoch\n",
        "                    best_model_wts = copy.deepcopy(s_model.state_dict())\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            epoch_time = epoch_end - epoch_start\n",
        "\n",
        "            print()\n",
        "\n",
        "    # Output runtime of model.\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Time {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    with open(log_file, 'a') as log:\n",
        "        log.write('Time {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        log.write('Best val Acc: {:4f} at epoch {}'.format(best_acc, best_epoch))\n",
        "\n",
        "    # Save weights of the best epoch model into training directory.\n",
        "    torch.save(best_model_wts, weight_file)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu12RTaTntSX"
      },
      "source": [
        "#Prepare data for training on each fold for mpl model.\n",
        "for fold, (train_idx, valid_idx) in enumerate(strat_kfold.split(X, y)):\n",
        "    print(\"FOLD {}\".format(fold+1))\n",
        "    \n",
        "    #Split training set into labelled and unlabelled set: labelled(40%)/unlabelled(60%)\n",
        "    #Split into labelled(50%)/unlabelled(50%) instead\n",
        "    labelled_idx = []\n",
        "    unlabelled_idx = []\n",
        "    a = train_idx\n",
        "    for idx in range(len(train_idx)):\n",
        "      index = train_idx[idx]\n",
        "      \n",
        "      # if (index % 5 == 0) or (index % 5 == 1):\n",
        "      if (index % 2 == 0):\n",
        "        labelled_idx.append(index)\n",
        "      else:\n",
        "        unlabelled_idx.append(index)\n",
        "    \n",
        "    labelled_df = label_df.iloc[labelled_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    unlabelled_df = label_df.iloc[unlabelled_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    valid_df = label_df.iloc[valid_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "\n",
        "    # to change directory\n",
        "    labelled_set = CellsDataset(labelled_df, \"Data_main_subset/images\")\n",
        "    unlabelled_set = CellsDataset(unlabelled_df, \"Data_main_subset/images\")\n",
        "    valid_set = CellsDataset(valid_df, \"Data_main_subset/images\")\n",
        "\n",
        "    # #Add augmentation data to original training set.\n",
        "    # aug_dir = \"../test_augment/\"\n",
        "    # aug_df = augment(rounds=aug_rounds, op_dir=aug_dir, labels=train_df)\n",
        "    # aug_set = CellsDataset(aug_df, aug_dir)\n",
        "    # train_aug_set = torch.utils.data.ConcatDataset([train_set, aug_set])\n",
        "\n",
        "    #Load dataset onto dataloader\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size,pin_memory=True, shuffle=True)\n",
        "    labeltrng_loader = torch.utils.data.DataLoader(labelled_set, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
        "    unlabeltrng_loader = torch.utils.data.DataLoader(unlabelled_set, batch_size=batch_size, shuffle=True)\n",
        "    # new format for dataloaders\n",
        "    # dataloaders = {'training_lab': valid_loader, 'training_unlab': train_loader, 'validation': valid_loader}\n",
        "    dataloaders = {'training': [labeltrng_loader, unlabeltrng_loader], 'validation': valid_loader}\n",
        "    dataset_sizes = {'training': [len(labeltrng_loader), len(unlabeltrng_loader)], 'validation': len(valid_set)}\n",
        "    class_names = [0, 1] # hard code labels based on labels in csv\n",
        "    \n",
        "    # train data on n epochs\n",
        "    trained_mplModel = train_metaModel(s_model, t_model, criterion, t_optimizer, s_optimizer, t_scheduler, s_scheduler, num_epochs=epochs)\n",
        "\n",
        "    #Delete augmented training directory before next training fold to save disk space.\n",
        "    #shutil.rmtree(aug_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qGsZdMOkBgg"
      },
      "source": [
        "## **Test Models** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGMBEVD43rR5"
      },
      "source": [
        "Output directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matHdgr03qXZ"
      },
      "source": [
        "weight_path = \"BaseModel/training_results/weights/weights_3Nov\"\n",
        "stats_path = \"BaseModel/training_results/stats\"\n",
        "version = 'v1' # change every round to prevent overriding of weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g50daWjVwsSW"
      },
      "source": [
        "## Put weights into ensembl model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNZefs3jVtlY"
      },
      "source": [
        "#Ensembl code for base model.\n",
        "# #Pull all files containing weights from each training fold.\n",
        "# # dir = \"/content/BaseModel/training_results/weights/\"\n",
        "# file_paths = os.listdir(weight_path)\n",
        "\n",
        "# #Store models weights in dictionary.\n",
        "# base_models = {}\n",
        "# for pth in file_paths:\n",
        "#   if 'pth' not in pth:\n",
        "#     continue\n",
        "#   path = f\"{weight_path}/{pth}\"\n",
        "#   model = BaseEfficientNet()\n",
        "#   model.load_state_dict(torch.load(path), strict = False)\n",
        "#   base_models[pth] = model\n",
        "\n",
        "# #Create base ensembl model.\n",
        "# baseModels = list(base_models.values())\n",
        "# base_ensembl_model = EnsblEfficientNet(baseModels)\n",
        "# #Set base ensembl model to evaluation mode\n",
        "# base_ensembl_model.eval()\n",
        "# #Freeze weights in ensembl model\n",
        "# for param in base_ensembl_model.parameters():\n",
        "#     param.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhPKD9qSh7mq"
      },
      "source": [
        "#Ensembl code for MPL model.\n",
        "#Pull all files containing weights from each training fold.\n",
        "dir = \"MPLModel/training_results/weights\"\n",
        "file_paths = os.listdir(dir)\n",
        "\n",
        "#Store models weights in dictionary.\n",
        "metapseudo_models = {}\n",
        "for file in file_paths:\n",
        "  path = dir + file\n",
        "  model = EfficientNet()\n",
        "  model.load_state_dict(torch.load(path), strict = False)\n",
        "  models[file.split('.')[0]] = model\n",
        "\n",
        "#Create MPL ensembl model.\n",
        "mpl_models = metapseudo_models.values()\n",
        "mpl_ensembl_model = EnsblEfficientNet(mpl_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO-xl0vUu82j"
      },
      "source": [
        "## Evaluating individual fold's model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geOIyZVAyfOG"
      },
      "source": [
        "weights = os.listdir(weight_path)\n",
        "\n",
        "# remove folders without pth extension\n",
        "for weight in weights:\n",
        "  if 'pth' not in weight:\n",
        "    weights.remove(weight)\n",
        "weights.sort()\n",
        "\n",
        "# Iterate through each fold's weights\n",
        "for weight in weights:\n",
        "  print(f\"Evaluating: {weight}\")\n",
        "  model = BaseEfficientNet()\n",
        "  model.load_state_dict(torch.load(f\"{weight_path}/{weight}\"), strict = False)\n",
        "  fold = weight.split('_')[0]\n",
        "  test_model(test_loader, model, stats_path=stats_path, fold=fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG4ZnNRdcAfP"
      },
      "source": [
        "# Data Augmentation #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PPcqBa9cH7Z"
      },
      "source": [
        "#Function for augmentation details.\n",
        "def augmentation(img):\n",
        "    img = torch.from_numpy(img).type(torch.DoubleTensor) # Change dtype from numpy to tensor.\n",
        "    img = torch.moveaxis(img, -1, 0)\n",
        "\n",
        "    affine_transformer = transforms.RandomAffine(degrees=(0, 360), # Rotates image.\n",
        "                                                 translate=(0.1, 0.1), #Offsets X/Y axes.\n",
        "                                                 scale=(0.95, 1.05)) #Scales size.\n",
        "\n",
        "    affine_img = affine_transformer(img).type(torch.LongTensor)\n",
        "    affine_img = torch.moveaxis(affine_img, 0, -1)\n",
        "\n",
        "    return affine_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqsYMt3cDlq5"
      },
      "source": [
        "## Augmentation Functions ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zseLQ4HoKjIB"
      },
      "source": [
        "## Testing Augmentation ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MUPapzJOD0t"
      },
      "source": [
        "#Create new directory to store augmentation test results.\n",
        "! mkdir test_augment\n",
        "\n",
        "#Copy 3 images each from ALL and Hem classes into test_augment directory.\n",
        "! find /content/ZB4171_LeukemiaImageClassification-Ongoing-/Data_Subset/training_data/fold_0/all -type f | head -3 | xargs cp -t /content/test_augment\n",
        "! find /content/ZB4171_LeukemiaImageClassification-Ongoing-/Data_Subset/training_data/fold_0/hem -type f | head -3 | xargs cp -t /content/test_augment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCMGfzT8usyX"
      },
      "source": [
        "!rename /content/test_augment/augmented/*.bmp /content/test_augment/augmented/*.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XwsbMTTQNzt"
      },
      "source": [
        "#Initialise input/output directory for testing.\n",
        "ip_dir = \"/content/test_augment/\"\n",
        "op_dir = \"/content/test_augment/augmented/\"\n",
        "labels = \"/content/ZB4171_LeukemiaImageClassification-Ongoing-/Data_main/labels.csv\"\n",
        "#Run data augmentation code. \n",
        "augment(rounds=5, ip_dir=ip_dir, op_dir=op_dir, labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L62Bsn3RnqF"
      },
      "source": [
        "# Visualise images created from augmentation test.\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "row = 2\n",
        "column = 6\n",
        "\n",
        "fig.add_subplot(row, column, 1)\n",
        "plt.imshow(cv2.imread(\"/content/test_augment/UID_28_34_5_all.bmp\"))\n",
        "plt.axis('off')\n",
        "plt.title(\"ALL_Org\")\n",
        "\n",
        "for i in range(5):\n",
        "  fig.add_subplot(row, column, i+2)\n",
        "  plt.imshow(cv2.imread(f\"/content/test_augment/augmented/UID_28_34_5_all_{i}.bmp\"))\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"ALL_{i}\")\n",
        "\n",
        "fig.add_subplot(row, column, 7)\n",
        "plt.imshow(cv2.imread(\"/content/test_augment/UID_H11_4_1_hem.bmp\"))\n",
        "plt.axis('off')\n",
        "plt.title(\"HEM_Org\")\n",
        "\n",
        "for j in range(5):\n",
        "  fig.add_subplot(row, column, j+8)\n",
        "  plt.imshow(cv2.imread(f\"/content/test_augment/augmented/UID_H11_4_1_hem_{j}.bmp\"))\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"HEM_{j}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5lg9xAiBRAc"
      },
      "source": [
        "#Augmentation function.\n",
        "\"\"\"\n",
        "    #Saves augmented image into op_dir given ip_dir of images and csv file/df.\n",
        "    :param rounds: no.of augmented images generated per image.\n",
        "    :param ip_dir: directory of image files.\n",
        "    :param op_dir: directory for augmented images.\n",
        "    :param labels: csv file or df for labels.\n",
        "    :return: saves temp_aug directory + aug_labels df.\n",
        "\"\"\"\n",
        "\n",
        "def augment(rounds, ip_dir=\"../Data_main/images/\", op_dir=\"../Data_main/temp_aug/\", labels=None):\n",
        "    \n",
        "    #Initialise input and output directory.\n",
        "    IP_DIR = ip_dir\n",
        "    OP_DIR = op_dir\n",
        "\n",
        "    #Create augmentation directory.\n",
        "    if not os.path.exists(OP_DIR):\n",
        "        os.makedirs(OP_DIR)\n",
        "\n",
        "    #Table to read images for augmentation.\n",
        "    try:\n",
        "        labels_df = pd.read_csv(labels) # For csv inputs.\n",
        "    except:\n",
        "        labels_df = labels # For df inputs.\n",
        "\n",
        "    #Create dataframe to attach labels to augmented images.\n",
        "    aug_l = []\n",
        "\n",
        "    #Iterate through labels_df to augment images.\n",
        "    for idx, row in tqdm(labels_df.iterrows()):\n",
        "        bmp = row[\"Patient_ID\"]\n",
        "        id = bmp.split('.')[0] # get name w/o .bmp extension\n",
        "        label = row[\"labels\"]\n",
        "        patient_no = row[\"Patient_no\"]\n",
        "\n",
        "        try:\n",
        "            img = plt.imread(IP_DIR + bmp)\n",
        "        except:\n",
        "            print('{} not in image folder'.format(bmp))\n",
        "            continue\n",
        "\n",
        "        for i in range(rounds):\n",
        "            augmented = augmentation(img).detach().numpy().astype('uint8')\n",
        "            aug_name = \"{}_{}\".format(i, id)\n",
        "\n",
        "            #Add augmented image info to dataframe.\n",
        "            aug_l.append([patient_no, aug_name, label])\n",
        "            #Save augmented image in bmp format.\n",
        "            plt.imsave('{}{}_{}.bmp'.format(OP_DIR, i, id), augmented)\n",
        "\n",
        "    aug_df = pd.DataFrame(aug_l, columns=[\"Patient_no\", \"Patient_ID\", \"labels\"])\n",
        "\n",
        "    return aug_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}