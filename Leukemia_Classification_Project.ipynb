{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZB4171 Leukemia Classification Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjsJM8Vd-ygM"
      },
      "source": [
        "# **Import Libraries** #\n",
        "Import any packages required for the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwIXkRDa-yOy"
      },
      "source": [
        "#For Import Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "# For Datasets and Dataloader\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.io import read_image\n",
        "\n",
        "#For EfficientNet Model Architecture\n",
        "! pip install efficientnet_pytorch\n",
        "import pandas as pd\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "#For ensemble models\n",
        "import collections\n",
        "\n",
        "#For Training & Testing\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMAdhYhVmiqz"
      },
      "source": [
        "#**Directories, Folders, Paths**\n",
        "Create directories & folders. Initialise file paths.\n",
        "\n",
        "**Upload LeukemiaData.zip (found on github) to mounted drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y7ttqOyAWz8"
      },
      "source": [
        "#Mount google drive to store datasets and results in.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avxKOeiRQqum"
      },
      "source": [
        "#Directory to store base EfficientNet model information\n",
        "!mkdir /content/baseEfficientNet\n",
        "!mkdir /content/baseEfficientNet/training_results\n",
        "!mkdir /content/baseEfficientNet/training_results/weights #Weights\n",
        "!mkdir /content/baseEfficientNet/training_results/stats #Training stats\n",
        "!mkdir /content/baseEfficientNet/testing_results/baseStats #Testing stats for base models\n",
        "!mkdir /content/baseEfficientNet/testing_results/ensembleStats #Testing stats for ensemble model\n",
        "\n",
        "#Directory to store Noisy Student model information\n",
        "!mkdir /content/noisyStudent\n",
        "!mkdir /content/noisyStudent/training_results\n",
        "!mkdir /content/noisyStudent/training_results/weights #Weights\n",
        "!mkdir /content/noisyStudent/training_results/stats #Training stats\n",
        "!mkdir /content/noisyStudent/testing_results/baseStats #Testing stats for base models\n",
        "!mkdir /content/noisyStudent/testing_results/ensembleStats #Testing stats for ensemble model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSfapOI8Bq0H"
      },
      "source": [
        "#File paths for training.\n",
        "train_labels = \"LeukemiaData/train_labels.csv\"\n",
        "train_images = \"LeukemiaData/train_images\"\n",
        "test_labels = \"LeukemiaData/testing/test_labels.csv\"\n",
        "test_images = \"LeukemiaData/testing/test_images\"\n",
        "\n",
        "#EfficientNet model\n",
        "baseEN_weights = \"baseEfficientNet/training_results/weights\"\n",
        "baseEN_stats = \"baseEfficientNet/training_results/stats\"\n",
        "baseEN_test = \"/content/baseEfficientNet/testing_results/baseStats\"\n",
        "ensembleEN_test = \"/content/baseEfficientNet/testing_results/ensembleStats\"\n",
        "\n",
        "#Noisy Student model\n",
        "baseNS_weights = \"/content/noisyStudent/training_results/weights\"\n",
        "baseNS_stats = \"/content/noisyStudent/training_results/stats\"\n",
        "baseNS_test = \"/content/noisyStudent/testing_results/baseStats\"\n",
        "ensembleNS_test = \"/content/noisyStudent/testing_results/ensembleStats\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhXY_Kze25mX"
      },
      "source": [
        "# **Import Leukemia Images** # "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmjz4ua2ZXJ"
      },
      "source": [
        "#Unzip file containing leukemia images.\n",
        "!unzip /content/drive/MyDrive/LeukemiaData.zip -d ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5e8RejP9F8"
      },
      "source": [
        "# **Classes & Class Utils** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv8x2xU3GiOS"
      },
      "source": [
        "##CellsDataset Class##\n",
        "Custom class for image dataset, required for usage of dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQcXP8RjP3vu"
      },
      "source": [
        "#Custom CellsDataset class for dataloader to process images.\n",
        "class CellsDataset(Dataset):\n",
        "    def __init__(self, label_file, img_dir):\n",
        "        #Initialise image directory\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "        #Initialise labels\n",
        "        try:\n",
        "            self.img_labels = pd.read_csv(label_file) #Csv files.\n",
        "        except:\n",
        "            self.img_labels = label_file #Df files.\n",
        "        \n",
        "        # remove rows without an image path\n",
        "        files = os.listdir(self.img_dir)\n",
        "        self.img_labels = self.img_labels.loc[self.img_labels['Patient_ID'].isin(files)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
        "        \n",
        "        #Normalises images according to pretrained EfficientNet-B0 values\n",
        "        transform_data = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        image = transform_data(image)\n",
        "        label = self.img_labels.iloc[idx, 2]\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFE00IKFIcfX"
      },
      "source": [
        "##EfficientNet Class##\n",
        "Custom class for base EfficientNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Zo3xXvcuUz"
      },
      "source": [
        "class BaseEfficientNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BaseEfficientNet,self).__init__()\n",
        "    self.model = EfficientNet.from_pretrained('efficientnet-b0') #Import pre-trained EfficientNet-B0.\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.fc = nn.Linear(1280, 2) \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.float()\n",
        "    x = self.model.extract_features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.flatten(start_dim=1)\n",
        "    x = self.dropout(x) #Randomly remove nodes to prevent overfitting\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1XIEhTcOBnN"
      },
      "source": [
        "## Ensemble Model Utils ##\n",
        "Helper functions to create ensemble model and perform ensemble voting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNqaKhHmHCjp"
      },
      "source": [
        "#Function to create ensemble model, returns a dictionary of {model name: model weights}.\n",
        "def createEnsemble(model_class, weights_path):\n",
        "    #Pull files containing base model weights.\n",
        "    file_paths = os.listdir(weights_path)\n",
        "\n",
        "    #Retrieve weights of each base model.\n",
        "    model_weights = list()\n",
        "    for pth in file_paths:\n",
        "        if 'pth' not in pth:\n",
        "          continue\n",
        "        path = f\"{weights_path}/{pth}\"\n",
        "        model = model_class\n",
        "        model.load_state_dict(torch.load(path, map_location = torch.device('cpu')), strict = False)\n",
        "        model_weights.append(model)\n",
        "\n",
        "    #Store base weights in dictionary.\n",
        "    models = dict()\n",
        "    models['model1'] = model_weights[0]\n",
        "    models['model2'] = model_weights[1]\n",
        "    models['model3'] = model_weights[2]\n",
        "    models['model4'] = model_weights[3]\n",
        "    models['model5'] = model_weights[4]\n",
        "\n",
        "    #Delete intermediate variables to save memory\n",
        "    del model_weights\n",
        "    del model\n",
        "\n",
        "    return models"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCHBUfE_q20S"
      },
      "source": [
        "#Function for ensemble prediction via majority voting classification.\n",
        "def get_ensembleVoting(ensemble_model, test_input):\n",
        "  #Initialise base models of ensemble model.\n",
        "  model1 = ensemble_model['model1']\n",
        "  model2 = ensemble_model['model2']\n",
        "  model3 = ensemble_model['model3']\n",
        "  model4 = ensemble_model['model4']\n",
        "  model5 = ensemble_model['model5']\n",
        "  \n",
        "  #Send models to CPU/GPU \n",
        "  model1.to(device)\n",
        "  model2.to(device)\n",
        "  model3.to(device)\n",
        "  model4.to(device)\n",
        "  model5.to(device)\n",
        "\n",
        "  #Set model to evaluation mode\n",
        "  model1.eval()\n",
        "  model2.eval()\n",
        "  model3.eval()\n",
        "  model4.eval()\n",
        "  model5.eval()\n",
        "  \n",
        "  #Obtain base model predictions.\n",
        "  preds = list()\n",
        "  model1_output = model1(test_input)\n",
        "  _, model1_pred = torch.max(model1_output.data, 1)\n",
        "  preds.append(model1_pred)\n",
        "\n",
        "  model2_output = model2(test_input)\n",
        "  _, model2_pred = torch.max(model2_output.data, 1)\n",
        "  preds.append(model2_pred)\n",
        "\n",
        "  model3_output = model3(test_input)\n",
        "  _, model3_pred = torch.max(model3_output.data, 1)\n",
        "  preds.append(model3_pred)\n",
        "\n",
        "  model4_output = model4(test_input)\n",
        "  _, model4_pred = torch.max(model4_output.data, 1)\n",
        "  preds.append(model4_pred)\n",
        "\n",
        "  model5_output = model5(test_input)\n",
        "  _, model5_pred = torch.max(model5_output.data, 1)\n",
        "  preds.append(model5_pred)\n",
        "\n",
        "  #Output final ensemble prediction.\n",
        "  votes = collections.Counter(preds) #Counts frequency of votes\n",
        "  common_vote = votes.most_common(1) #Retrive most common vote\n",
        "  final_pred = common_vote[0][0]\n",
        "\n",
        "  return final_pred"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJWxC2x4E92A"
      },
      "source": [
        "# **Training Base Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpvcMMioFnts"
      },
      "source": [
        "## **Initialise Training Parameters** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6wWPxXsFr4U"
      },
      "source": [
        "#Training parameters.\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9 # SGD momentum\n",
        "step_size = 7 # scheduler step size\n",
        "gamma = 0.1 # learning rate decay\n",
        "seed = torch.manual_seed(42)\n",
        "batch_size = 5\n",
        "num_workers = 5\n",
        "CV_states = 42\n",
        "num_fold = 5\n",
        "epochs = 50\n",
        "\n",
        "#EfficientNet and Noisy Student model parameters.\n",
        "base_model = BaseEfficientNet()\n",
        "criterion = nn.CrossEntropyLoss() #Loss function.\n",
        "optimizer = optim.SGD(base_model.parameters(), \n",
        "                      lr=learning_rate, \n",
        "                      momentum=momentum) #Stochastic Gradient descent.\n",
        "scheduler = StepLR(optimizer, \n",
        "                   step_size=step_size, \n",
        "                   gamma=gamma) #Decay learning rate.\n",
        "patience = 50 #Early stopping.\n",
        "\n",
        "#Specify GPU/CPU usage.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "print(f\"Device in use: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9vbQZ0rFc9y"
      },
      "source": [
        "## **Initialise K-Fold CVs** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnPgVJOi0hGJ"
      },
      "source": [
        "# Define stratified K-fold cross-validation function.\n",
        "strat_kfold = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=CV_states)\n",
        "\n",
        "#Obtain indexes for stratified k fold.\n",
        "trainLabel_df = pd.read_csv(train_labels)\n",
        "X = np.zeros(len(trainLabel_df))\n",
        "y = trainLabel_df['labels']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzdy4swVP10_"
      },
      "source": [
        "## **Training base EfficientNet Model** ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOiyt7DRA8C2"
      },
      "source": [
        "#Training function for EfficientNet model.\n",
        "def efficientNet_training(model, criterion, optimizer, scheduler, num_epochs=epochs, fold=0): \n",
        "    # specify output files\n",
        "    log_file = f\"{baseEN_stats}/fold{fold+1}_log.log\"\n",
        "    weight_file = f\"{baseEN_weights}/fold{fold+1}_weights.pth\"\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    #Initialise model.\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    #Epoch loop.\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_no = epoch+1\n",
        "        print('Epoch {}/{}'.format(epoch_no, num_epochs))\n",
        "        print('-' * 10)\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Epoch {}/{}, '.format(epoch_no, num_epochs))\n",
        "        \n",
        "        for phase in ['training', 'validation']:\n",
        "            if phase == 'training':\n",
        "                model.train() #Training mode.\n",
        "                \n",
        "            else:\n",
        "                model.eval() #Validation mode.\n",
        "\n",
        "            p_count = 0\n",
        "            lowest_loss = 0.0\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over images.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                #Set parameter gradients to 0.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #Training phase.\n",
        "                #Iterate forward.\n",
        "                with torch.set_grad_enabled(phase == 'training'):\n",
        "                    outputs = model(inputs) #Model prediction.\n",
        "                    soft_preds = torch.softmax(outputs, dim = -1) #Soft labels\n",
        "                    max_prob, hard_preds = torch.max(soft_preds, dim = -1) #Hard labels\n",
        "                    loss = criterion(soft_preds, labels) #Calculate loss.\n",
        "\n",
        "                    #Backward propagate. \n",
        "                    if phase == 'training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #Training statistics.\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(hard_preds == labels.data)\n",
        "            \n",
        "            if phase == 'training':\n",
        "                scheduler.step()\n",
        "\n",
        "            #Epoch statistics.\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            #Output training statistics.\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            with open(log_file, 'a') as log:\n",
        "              log.write('{} Loss: {:.4f} Acc: {:.4f}, '.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "              \n",
        "            #Validation phase.\n",
        "            if phase == 'validation':\n",
        "              with open(log_file, 'a') as log:\n",
        "                  log.write('\\n')\n",
        "\n",
        "              #Add patience for early stopping.\n",
        "              if (lowest_loss <= epoch_loss):\n",
        "                p_count += 1\n",
        "                if p_count > patience:\n",
        "                  print(f'val loss did not decrease after {patience} epochs')\n",
        "                  break\n",
        "\n",
        "              if lowest_loss > epoch_loss:\n",
        "                p_count = 0\n",
        "                lowest_loss = epoch_loss\n",
        "\n",
        "              #Save model weights if epoch gives best accuracy.\n",
        "              if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_epoch = epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        #Output epoch statistics.\n",
        "        epoch_end = time.time()\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        print('Epoch Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "        print()\n",
        "    \n",
        "    #Output runtime.\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Time {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    with open(log_file, 'a') as log:\n",
        "      log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          time_elapsed // 60, time_elapsed % 60))\n",
        "      log.write('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    #Save weights of the best epoch model as file.\n",
        "    torch.save(best_model_wts, weight_file)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cxR7lc-P0ZV"
      },
      "source": [
        "#Fold code for EfficientNet training.\n",
        "for fold, (train_idx, valid_idx) in enumerate(strat_kfold.split(X, y)):\n",
        "    fold_no = fold+1\n",
        "    print(f\"FOLD {fold_no}\") # Keep track of fold number \n",
        "\n",
        "    #Convert training and validation data to custom class.\n",
        "    train_df = trainLabel_df.iloc[train_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    valid_df = trainLabel_df.iloc[valid_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    train_set = CellsDataset(train_df, train_images)\n",
        "    valid_set = CellsDataset(valid_df, train_images)\n",
        "\n",
        "    #Load training and validation sets to dataloader.\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size, \n",
        "                              num_workers=num_workers, shuffle=True)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, \n",
        "                              num_workers=num_workers, shuffle=True)\n",
        "    dataloaders = {'training': train_loader, 'validation': valid_loader}\n",
        "    dataset_sizes = {'training': len(train_set), 'validation': len(valid_set)}\n",
        "    class_names = [0, 1] #Binary class labels.\n",
        "    \n",
        "    #Train EfficientNet model.\n",
        "    efficientNet_training(base_model, criterion, optimizer, scheduler, \n",
        "                          num_epochs = epochs, fold = num_fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7II0EPGj04BY"
      },
      "source": [
        "## **Training Noisy Student** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xqxCKPd6SJ8"
      },
      "source": [
        "#Training function for Noisy Student model.\n",
        "def noisyStudentTraining(model, dataloader, dataset_sizes, criterion, optimizer, scheduler, num_epochs = 50, fold = 0):\n",
        "    # specify output files\n",
        "    log_file = f\"{baseNS_stats}/fold{fold+1}_log.log\"\n",
        "    weight_file = f\"{baseNS_weights}/fold{fold+1}_weights.pth\"\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    #Initialise model.\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    #Epoch loop.\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        epoch_no = epoch+1\n",
        "        print('Epoch {}/{}'.format(epoch_no, num_epochs))\n",
        "        print('-' * 10)\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Epoch {}/{}, '.format(epoch_no, num_epochs))\n",
        "        \n",
        "        for phase in ['training', 'validation']:\n",
        "            if phase == 'training':\n",
        "                model.train()  #Training mode.\n",
        "                \n",
        "            else:\n",
        "                model.eval()   #Validation mode.\n",
        "\n",
        "            p_count = 0\n",
        "            lowest_loss = 0.0\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            #Iterate over images.\n",
        "            for inputs, labels in dataloader[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                #Set parameter gradients to 0.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #Training phase: Iterate forward.\n",
        "                with torch.set_grad_enabled(phase == 'training'):\n",
        "                    outputs = model(inputs) #Model prediction.\n",
        "                    soft_preds = torch.softmax(outputs, dim = -1) #Soft labels.\n",
        "                    max_prob, hard_preds = torch.max(soft_preds, dim = -1) #Hard labels.\n",
        "                    loss = criterion(soft_preds, labels) #Calculate loss.\n",
        "\n",
        "                    #Backward propagate. \n",
        "                    if phase == 'training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                #Training statistics.\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(hard_preds == labels.data)\n",
        "            \n",
        "            if phase == 'training':\n",
        "                scheduler.step()\n",
        "\n",
        "            #Epoch statistics.\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            #Output training statistics.\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            with open(log_file, 'a') as log:\n",
        "              log.write('{} Loss: {:.4f} Acc: {:.4f}, '.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "              \n",
        "            #Validation phase.\n",
        "            if phase == 'validation':\n",
        "              with open(log_file, 'a') as log:\n",
        "                  log.write('\\n')\n",
        "\n",
        "              #Add patience for early stopping.\n",
        "              if (lowest_loss <= epoch_loss):\n",
        "                p_count += 1\n",
        "                if p_count > patience:\n",
        "                  print(f'val loss did not decrease after {patience} epochs')\n",
        "                  break\n",
        "\n",
        "              if lowest_loss > epoch_loss:\n",
        "                p_count = 0\n",
        "                lowest_loss = epoch_loss\n",
        "\n",
        "              #Save model weights if epoch gives best accuracy.\n",
        "              if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_epoch = epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        #Output epoch statistics.\n",
        "        epoch_end = time.time()\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        print('Epoch Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "        with open(log_file, 'a') as log:\n",
        "          log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          epoch_time // 60, epoch_time % 60))\n",
        "        print()\n",
        "    \n",
        "    #Output runtime.\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Time {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    #Save accuracy and runtime as log file.\n",
        "    with open(log_file, 'a') as log:\n",
        "      log.write('Time {:.0f}m {:.0f}s\\n'.format(\n",
        "          time_elapsed // 60, time_elapsed % 60))\n",
        "      log.write('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    #Save weights of the best epoch model as file.\n",
        "    torch.save(best_model_wts, weight_file)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukY3zI11DJC"
      },
      "source": [
        "#Fold code for Noisy Student training.\n",
        "for fold, (train_idx, valid_idx) in enumerate(strat_kfold.split(X, y)):\n",
        "    fold_no = fold+1\n",
        "    print(\"FOLD {}\".format(fold_no+1))\n",
        "    \n",
        "    #Split training set into labelled(50%) & unlabelled(50%) sets.\n",
        "    labelled_idx = []\n",
        "    unlabelled_idx = []\n",
        "\n",
        "    for idx in range(len(train_idx)):\n",
        "      split_idx = train_idx[idx]\n",
        "      if (split_idx % 2 == 0):\n",
        "        labelled_idx.append(split_idx)\n",
        "      else:\n",
        "        unlabelled_idx.append(split_idx)\n",
        "    \n",
        "    #Convert labelled, unlabelled and validation sets to custom class.\n",
        "    labelled_df = trainLabel_df.iloc[labelled_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    unlabelled_df = trainLabel_df.iloc[unlabelled_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    pseudolabel_df = unlabelled_df[[\"Patient_no\", \"Patient_ID\"]]\n",
        "    valid_df = trainLabel_df.iloc[valid_idx][[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "    valid_set = CellsDataset(valid_df, train_images)\n",
        "    labelled_set = CellsDataset(labelled_df, train_images)\n",
        "    unlabelled_set = CellsDataset(unlabelled_df, train_images)\n",
        "\n",
        "    #Load labelled, unlabelled and validation sets to dataloader.\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers, \n",
        "                              pin_memory=True, shuffle=True)\n",
        "    label_loader = DataLoader(labelled_set, batch_size=batch_size, num_workers=num_workers, \n",
        "                              pin_memory=True, shuffle=True)\n",
        "    unlabel_loader = DataLoader(unlabelled_set, batch_size=batch_size, num_workers=num_workers, \n",
        "                                pin_memory=True, shuffle=True)\n",
        "    label_dataloader = {'training': label_loader, 'validation': valid_loader}\n",
        "    dataset_sizes_l = {'training': len(labelled_set), 'validation': len(valid_set)}\n",
        "    class_names = [0, 1] #Binary class labels.\n",
        "\n",
        "    #Noisy student loop.\n",
        "    NS_iterations = 3\n",
        "    for iteration in range(NS_iterations):\n",
        "      if iteration == 0:\n",
        "        print(\"TRAINING TEACHER:\")\n",
        "        noisyStudentTraining(base_model, label_dataloader, dataset_sizes_l, \n",
        "                             criterion, optimizer, scheduler, \n",
        "                             num_epochs = epochs, fold=num_fold) #Train first teacher model on labelled images.\n",
        "\n",
        "      else:\n",
        "        base_model.eval()\n",
        "\n",
        "        #Predict pseudolabels from unlabelled images.\n",
        "        infer_pseudolabels = []\n",
        "        for inputs, _ in unlabel_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            with torch.no_grad():\n",
        "              output = base_model(inputs) #Teacher model prediction.\n",
        "              soft_preds = torch.softmax(output, dim = -1) #Soft labels.\n",
        "              max_prob, hard_preds = torch.max(soft_preds, dim = -1) #Hard labels.\n",
        "              hard_preds = hard_preds.cpu().detach().numpy()\n",
        "              infer_pseudolabels.extend(hard_preds)\n",
        "\n",
        "        #Attach pseudolabels with unlabelled images and send to dataloader.\n",
        "        pseudolabel_df['labels'] = infer_pseudolabels\n",
        "        pseudolabel_set = CellsDataset(pseudolabel_df, unlabelled_images)\n",
        "        pseudolabel_loader = torch.utils.data.DataLoader(pseudolabel_set, batch_size=batch_size, shuffle=True)\n",
        "        pseudo_dataloader = {'training': pseudolabel_loader, 'validation': valid_loader}\n",
        "        dataset_sizes_u = {'training': len(pseudolabel_set), 'validation': len(valid_set)}\n",
        "\n",
        "        print(f\"TRAINING STUDENT ITERATION: {iteration}\")\n",
        "        noisyStudentTraining(base_model, pseudo_dataloader, dataset_sizes_u, \n",
        "                             criterion, optimizer, scheduler, \n",
        "                             num_epochs = epochs, fold = num_fold) #Train student model on labelled images.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FwLc0PAKtsn"
      },
      "source": [
        "## **Create Ensemble Models** ##\n",
        "Combine base models after training to form ensemble models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oxc4-cMIlwE"
      },
      "source": [
        "#Create ensemble models from base models.\n",
        "ensembleEN_model = createEnsemble(BaseEfficientNet(), baseEN_weights)\n",
        "ensembleNS_model = createEnsemble(BaseEfficientNet(), baseNS_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiAT98O8qwjj"
      },
      "source": [
        "# **Testing and Evaluation** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCYXoY-VOH2l"
      },
      "source": [
        "## **Testing Function** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hroUC0mrOBXl"
      },
      "source": [
        "#Testing function for base and ensemble models.\n",
        "def test_model(model, stats_path, ensemble=False, fold = ''):\n",
        "  #Message to indicate base or ensemble model evaluation.\n",
        "  if ensemble == False:\n",
        "    print(\"Evaluating base models...\")\n",
        "  else:\n",
        "    print(\"Evaluating ensemble model...\")\n",
        "  \n",
        "  #Create empty confusion matrix.\n",
        "  confusion_matrix = np.zeros((2,2), dtype=int)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    #Iterate over testing images.\n",
        "    for inputs, labels in test_dataloader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "    \n",
        "      if ensemble == False:\n",
        "          output = model(inputs) #Base model prediction.\n",
        "          _, prediction = torch.max(output.data, 1) #Hard labels.\n",
        "\n",
        "      if ensemble == True:\n",
        "          prediction = get_ensembleVoting(model, inputs) #Ensemble voting.\n",
        "\n",
        "      #Add predictions to confusion matrix.\n",
        "      for j in range(inputs.size()[0]): \n",
        "          if prediction[j]==1 and labels[j]==1:\n",
        "              term='TP'\n",
        "              confusion_matrix[0][0]+=1\n",
        "\n",
        "          elif prediction[j]==1 and labels[j]==0:\n",
        "              term='FP'\n",
        "              confusion_matrix[1][0]+=1\n",
        "\n",
        "          elif prediction[j]==0 and labels[j]==1:\n",
        "              term='FN'\n",
        "              confusion_matrix[0][1]+=1\n",
        "        \n",
        "          elif prediction[j]==0 and labels[j]==0:\n",
        "              term='TN'\n",
        "              confusion_matrix[1][1]+=1\n",
        "\n",
        "      #Obtain results from confusion matrix.\n",
        "      TP = confusion_matrix[0][0]\n",
        "      FP = confusion_matrix[1][0]\n",
        "      FN = confusion_matrix[0][1]\n",
        "      TN = confusion_matrix[1][1]\n",
        "\n",
        "      #Test statistics.\n",
        "      accuracy = 100*(TP+TN)/ (TP+FP+TN+FN)\n",
        "      sensitivity = (100*TP)/(TP+FP)\n",
        "      specificity = (100*TN)/(TN+FN)\n",
        "      PPV = (100*TP)/(TP+FN)\n",
        "      NPV = (100*TN)/(TN+FP)\n",
        "      F1 = 2*(PPV*sensitivity)/(PPV+sensitivity) \n",
        "\n",
        "    #Print test statistics.\n",
        "    print('-----------------------')\n",
        "    print('PREDICTION STATISTICS')\n",
        "    print('-----------------------')\n",
        "    print('True Positives: ' + str(TP))\n",
        "    print('False Positives: ' + str(FP))\n",
        "    print('False Negatives: ' + str(FN))\n",
        "    print('True Negatives: ' + str(TN))\n",
        "\n",
        "    print('-----------------------')\n",
        "    print('EVALUATION STATISTICS')\n",
        "    print('-----------------------')\n",
        "    print('Accuracy: %f %%' % (accuracy))\n",
        "    print('Sensitivity: %f %%' % (sensitivity))\n",
        "    print('Specificity: %f %%' % (specificity))\n",
        "    print('PPV: %f %%' % (PPV))\n",
        "    print('NPV: %f %%' % (NPV))\n",
        "    print('F1 Score: %f %%' % (F1))\n",
        "\n",
        "    #Save test statistics as log files.\n",
        "    stat_file = f\"{stats_path}_stats.log\"\n",
        "    if ensemble:\n",
        "      with open(stat_file, 'a') as log:\n",
        "        log.write('ENSEMBLE MODEL:' + '\\n')\n",
        "    else:\n",
        "      with open(stat_file, 'a') as log:\n",
        "        log.write(fold + '\\n')\n",
        "        \n",
        "    with open(stat_file, 'a') as log:\n",
        "      log.write(\n",
        "      'True Positives: ' + str(TP) + '\\n'\n",
        "      'False Positives: ' + str(FP) + '\\n'\n",
        "      'False Negatives: ' + str(FN) + '\\n'\n",
        "      'True Negatives: ' + str(TN) + '\\n' + '\\n'\n",
        "      'Accuracy: %f %%' % (accuracy) +'\\n'\n",
        "      'Sensitivity: %f %%' % (sensitivity) + '\\n'\n",
        "      'Specificity: %f %%' % (specificity) + '\\n'\n",
        "      'PPV: %f %%' % (PPV) + '\\n'\n",
        "      'NPV: %f %%' % (NPV) + '\\n'\n",
        "      'F1 Score: %f %%' % (F1) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoiT57bT1TqL"
      },
      "source": [
        "## Evaluating Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwuleombgYWN"
      },
      "source": [
        "#Convert testing set to custom class.\n",
        "test_df = pd.read_csv(test_labels)[[\"Patient_no\", \"Patient_ID\", \"labels\"]]\n",
        "test_set = CellsDataset(test_df, test_images)\n",
        "\n",
        "#Send testing set to dataloader.\n",
        "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#Evaluate base EfficientNet models.\n",
        "EN_weights = os.listdir(baseEN_weights)\n",
        "for weight in EN_weights:\n",
        "  pth = weight_dir + '/'+ weight\n",
        "  model = BaseEfficientNet()\n",
        "  model.load_state_dict(torch.load(pth))\n",
        "  model.to(device) #load model to gpu\n",
        "\n",
        "  path = baseEN_stats + '/' + weight.split('.')[0]\n",
        "  test_model(model, stats_path=path, ensemble=False)\n",
        "\n",
        "#Evaluate base Noisy Student models.\n",
        "NS_weights = os.listdir(baseNS_weights)\n",
        "for weight in NS_weights:\n",
        "  pth = weight_dir + '/'+ weight\n",
        "  model = BaseEfficientNet()\n",
        "  model.load_state_dict(torch.load(pth))\n",
        "  model.to(device) #load model to gpu\n",
        "\n",
        "  path = baseNS_stats + '/' + weight.split('.')[0]\n",
        "  test_model(model, stats_path=path, ensemble=False)\n",
        "\n",
        "#Evaluate ensemble models.\n",
        "test_model(ensembleEN_model, stats_path=ensembleEN_test, ensemble=True)\n",
        "test_model(ensembleNS_model, stats_path=ensembleNS_test, ensemble=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}